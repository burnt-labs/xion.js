name: Integration Tests

on:
  pull_request:
  workflow_dispatch:
    inputs:
      test_environment:
        description: 'Integration test environment'
        required: false
        type: choice
        options:
          - testnet
          - mainnet
        default: 'testnet'
      aa_api_url:
        description: 'Custom AA API URL to use for integration tests'
        required: false
        type: string
  push:
    branches:
      - main
      - develop

concurrency: ${{ github.workflow }}-${{ github.ref }}

jobs:
  integration-test:
    name: Run integration tests (${{ github.event.inputs.test_environment || 'testnet' }})
    strategy:
      matrix:
        os: [ubuntu-latest]
        node-version: [lts/*]
        pnpm-version: [8.9.0]
    runs-on: ${{ matrix.os }}
    steps:
      - name: â¬‡ï¸ Checkout
        uses: actions/checkout@v4

      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: ðŸ¥¡ Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ matrix.pnpm-version }}

      - name: ðŸ”„ Install Dependencies
        run: pnpm install

      - name: ðŸ”¨ Generate AA API Types
        run: pnpm generate:types

      - name: ðŸ—ï¸ Build
        run: pnpm build

      - name: ðŸ“ Load Test Configuration
        id: config
        run: |
          # Install jq if not available
          if ! command -v jq &> /dev/null; then
            sudo apt-get update && sudo apt-get install -y jq
          fi

          TEST_ENV="${{ github.event.inputs.test_environment || 'testnet' }}"

          echo "test_environment=$TEST_ENV" >> $GITHUB_OUTPUT

          # Load configuration from JSON file
          chmod +x .github/scripts/load-test-config.sh
          source .github/scripts/load-test-config.sh "$TEST_ENV"

          # Override AA-API URL if specified
          if [ -n "${{ github.event.inputs.aa_api_url }}" ]; then
            FINAL_AA_API_URL="${{ github.event.inputs.aa_api_url }}"
            echo "Using custom AA-API URL: $FINAL_AA_API_URL"
          else
            FINAL_AA_API_URL="$XION_AA_API_URL"
            echo "Using configured AA-API URL: $FINAL_AA_API_URL"
          fi

          echo "aa_api_url=$FINAL_AA_API_URL" >> $GITHUB_OUTPUT
          echo "XION_AA_API_URL=$FINAL_AA_API_URL" >> $GITHUB_ENV

      - name: ðŸ§ª Run Integration Tests
        id: run-tests
        env:
          # Test environment from config file
          TEST_ENVIRONMENT: ${{ steps.config.outputs.test_environment }}

          # Testnet configuration (loaded from test-environments.json)
          XION_TESTNET_CHAIN_ID: ${{ env.XION_TESTNET_CHAIN_ID }}
          XION_TESTNET_RPC_URL: ${{ env.XION_TESTNET_RPC_URL }}
          XION_TESTNET_REST_URL: ${{ env.XION_TESTNET_REST_URL }}
          XION_TESTNET_GAS_PRICE: ${{ env.XION_TESTNET_GAS_PRICE }}
          XION_TESTNET_FEE_GRANTER: ${{ env.XION_TESTNET_FEE_GRANTER }}
          XION_TESTNET_AA_API_URL: ${{ steps.config.outputs.test_environment == 'testnet' && steps.config.outputs.aa_api_url || env.XION_TESTNET_AA_API_URL }}
          XION_TESTNET_TREASURY_ADDRESS: ${{ env.XION_TESTNET_TREASURY_ADDRESS }}
          XION_TESTNET_INDEXER_URL: ${{ secrets.XION_TESTNET_INDEXER_URL }}
          XION_TESTNET_TREASURY_INDEXER_URL: ${{ secrets.XION_TESTNET_TREASURY_INDEXER_URL }}

          # Mainnet configuration (loaded from test-environments.json, sensitive URLs from secrets)
          XION_MAINNET_CHAIN_ID: ${{ env.XION_MAINNET_CHAIN_ID }}
          XION_MAINNET_RPC_URL: ${{ env.XION_MAINNET_RPC_URL }}
          XION_MAINNET_REST_URL: ${{ env.XION_MAINNET_REST_URL }}
          XION_MAINNET_GAS_PRICE: ${{ env.XION_MAINNET_GAS_PRICE }}
          XION_MAINNET_FEE_GRANTER: ${{ env.XION_MAINNET_FEE_GRANTER }}
          XION_MAINNET_TREASURY_ADDRESS: ${{ env.XION_MAINNET_TREASURY_ADDRESS }}
          XION_MAINNET_AA_API_URL: ${{ secrets.XION_MAINNET_AA_API_URL }}
          XION_MAINNET_INDEXER_URL: ${{ secrets.XION_MAINNET_INDEXER_URL }}
          XION_MAINNET_TREASURY_INDEXER_URL: ${{ secrets.XION_MAINNET_TREASURY_INDEXER_URL }}

          # Test configuration
          TEST_TIMEOUT: ${{ env.TEST_TIMEOUT }}
          INTEGRATION_TEST_TIMEOUT: ${{ env.INTEGRATION_TEST_TIMEOUT }}
          NODE_ENV: test
          CI: true
        run: |
          echo "ðŸ§ª Running integration tests"
          echo "Environment: $TEST_ENVIRONMENT"
          echo "AA-API URL: ${{ steps.config.outputs.aa_api_url }}"
          echo "Chain ID: $XION_TESTNET_CHAIN_ID"
          echo "RPC URL: $XION_TESTNET_RPC_URL"
          echo ""

          # Run integration tests and capture results
          set +e  # Don't exit on error

          pnpm --filter @burnt-labs/account-management test:integration 2>&1 | tee account-management-results.txt
          ACCOUNT_MGMT_EXIT=$?

          pnpm --filter @burnt-labs/abstraxion test:integration 2>&1 | tee abstraxion-results.txt
          ABSTRAXION_EXIT=$?

          # Parse test results to count failures (for workflow control)
          TOTAL_FAILED=0

          # Extract just the number of failed tests using simple grep
          # Format: "Tests  9 failed | 169 passed | 4 skipped (182)"
          if [ -f account-management-results.txt ]; then
            FAILED=$(grep -a 'Tests' account-management-results.txt 2>/dev/null | grep -oP '\d+(?=\s+failed)' | head -1 || echo "0")
            TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
          fi

          if [ -f abstraxion-results.txt ]; then
            FAILED=$(grep -a 'Tests' abstraxion-results.txt 2>/dev/null | grep -oP '\d+(?=\s+failed)' | head -1 || echo "0")
            TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
          fi

          # Extract test summary for PR comment (last ~20 lines of each output)
          TEST_SUMMARY=""

          if [ -f account-management-results.txt ]; then
            ACCT_MGMT_SUMMARY=$(grep -a -E "Test Files|Tests|Time" account-management-results.txt 2>/dev/null | tail -5 || echo "No summary available")
            TEST_SUMMARY="${TEST_SUMMARY}**@burnt-labs/account-management:**\n\`\`\`\n${ACCT_MGMT_SUMMARY}\n\`\`\`\n\n"
          fi

          if [ -f abstraxion-results.txt ]; then
            ABSTRAXION_SUMMARY=$(grep -a -E "Test Files|Tests|Time" abstraxion-results.txt 2>/dev/null | tail -5 || echo "No summary available")
            TEST_SUMMARY="${TEST_SUMMARY}**@burnt-labs/abstraxion:**\n\`\`\`\n${ABSTRAXION_SUMMARY}\n\`\`\`"
          fi

          # Save summary for PR comment
          echo "test_summary<<EOF" >> $GITHUB_OUTPUT
          echo -e "$TEST_SUMMARY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "total_failed=$TOTAL_FAILED" >> $GITHUB_OUTPUT

          echo ""
          echo "ðŸ“Š Test Summary:"
          echo "  Total Failed: $TOTAL_FAILED"

          # Fail if more than 10 tests failed
          if [ $TOTAL_FAILED -gt 10 ]; then
            echo "âŒ More than 10 tests failed ($TOTAL_FAILED), marking workflow as failed"
            echo "test_status=failed" >> $GITHUB_OUTPUT
            exit 1
          elif [ $TOTAL_FAILED -gt 0 ]; then
            echo "âš ï¸  Some tests failed ($TOTAL_FAILED), but within acceptable threshold (â‰¤10)"
            echo "test_status=warning" >> $GITHUB_OUTPUT
          else
            echo "âœ… All tests passed!"
            echo "test_status=success" >> $GITHUB_OUTPUT
          fi

          set -e

      - name: ðŸ’¬ Post Test Results Comment
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const testSummary = '${{ steps.run-tests.outputs.test_summary }}';
            const totalFailed = parseInt('${{ steps.run-tests.outputs.total_failed }}', 10) || 0;
            const testStatus = '${{ steps.run-tests.outputs.test_status }}' || 'unknown';

            // Skip if no test summary available
            if (!testSummary || testSummary.trim() === '') {
              console.log('No test results available, skipping comment');
              return;
            }

            let emoji = 'âœ…';
            let statusText = 'All tests passed';

            if (testStatus === 'failed') {
              emoji = 'âŒ';
              statusText = `Too many failures (${totalFailed} > 10)`;
            } else if (testStatus === 'warning') {
              emoji = 'âš ï¸';
              statusText = `Some tests failed (${totalFailed} â‰¤ 10)`;
            }

            const noteText = totalFailed > 0
              ? '\n\n> **Note:** Some integration test failures are expected due to testnet instability (network issues, indexer delays, fee grant limits, etc.). The workflow only fails if more than 10 tests fail.'
              : '';

            const body = '## ' + emoji + ' Integration Test Results\n\n' +
              '**Status:** ' + statusText + '\n\n' +
              testSummary +
              noteText + '\n\n' +
              '---\n' +
              '*Environment: ${{ steps.config.outputs.test_environment }}* | *AA-API: ${{ steps.config.outputs.aa_api_url }}*';

            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
